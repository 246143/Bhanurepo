{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d39fbf",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d230828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:\n",
      " ['aba' 'act' 'against' 'aware' 'be' 'broadcasting' 'calls' 'community'\n",
      " 'decides' 'defamation' 'fire' 'for' 'infrastructure' 'licence' 'must'\n",
      " 'of' 'protection' 'witnesses']\n",
      "Bow Matrix:\n",
      "\n",
      "[[1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0]\n",
      " [0 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 1]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text=['aba decides against community broadcasting licence','act fire witnesses must be aware of defamation',\n",
    "'a g calls for infrastructure protection']\n",
    "\n",
    "      \n",
    "vectorizer=CountVectorizer()\n",
    "\n",
    "      \n",
    "x=vectorizer.fit_transform(text)\n",
    "\n",
    "feature_names=vectorizer.get_feature_names_out()\n",
    "    \n",
    "print('Feature names:\\n',feature_names)\n",
    "      \n",
    "print('Bow Matrix:\\n')\n",
    "      \n",
    "print(x.toarray())\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144646ed",
   "metadata": {},
   "source": [
    "# N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf3d5fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:call for ethanol blend fuel to go ahead\n",
      "generated 2-grams:\n",
      "('call', 'for')\n",
      "('for', 'ethanol')\n",
      "('ethanol', 'blend')\n",
      "('blend', 'fuel')\n",
      "('fuel', 'to')\n",
      "('to', 'go')\n",
      "('go', 'ahead')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "\n",
    "text='call for ethanol blend fuel to go ahead'\n",
    "\n",
    "tokens=nltk.word_tokenize(text)\n",
    "n=2\n",
    "bigrams=list(ngrams(tokens,n))\n",
    "\n",
    "print(f\"Original text:{text}\")\n",
    "print(f\"generated {n}-grams:\")\n",
    "\n",
    "for gram in bigrams:\n",
    "    print(gram)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d4375c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:barca take record as robson celebrates birthday in\n",
      "generated 5-grams:\n",
      "('barca', 'take', 'record', 'as', 'robson')\n",
      "('take', 'record', 'as', 'robson', 'celebrates')\n",
      "('record', 'as', 'robson', 'celebrates', 'birthday')\n",
      "('as', 'robson', 'celebrates', 'birthday', 'in')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "\n",
    "text='barca take record as robson celebrates birthday in'\n",
    "\n",
    "tokens=nltk.word_tokenize(text)\n",
    "n=5\n",
    "bigrams=list(ngrams(tokens,n))\n",
    "\n",
    "print(f\"Original text:{text}\")\n",
    "print(f\"generated {n}-grams:\")\n",
    "\n",
    "for gram in bigrams:\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cc77f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names(TF-IDF):]\n",
      " ['affect' 'air' 'attacks' 'australian' 'big' 'businesses' 'championship'\n",
      " 'cycling' 'for' 'hopes' 'launceston' 'nz' 'prepare' 'should' 'strike'\n",
      " 'terrorist' 'to' 'travellers']\n",
      "TF-IDF Matrix:\n",
      " [[0.37796447 0.37796447 0.         0.37796447 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.37796447\n",
      "  0.         0.         0.37796447 0.         0.37796447 0.37796447]\n",
      " [0.         0.         0.         0.         0.42339448 0.\n",
      "  0.42339448 0.42339448 0.32200242 0.42339448 0.42339448 0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.42339448 0.         0.         0.42339448\n",
      "  0.         0.         0.32200242 0.         0.         0.\n",
      "  0.42339448 0.42339448 0.         0.42339448 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text=['air nz strike to affect australian travellers','big hopes for launceston cycling championship','businesses should prepare for terrorist attacks']\n",
    "\n",
    "tfidf=TfidfVectorizer()\n",
    "x_tfidf=tfidf.fit_transform(text)\n",
    "\n",
    "features_names_tfidf=tfidf.get_feature_names_out()\n",
    "\n",
    "print('Feature Names(TF-IDF):]\\n',features_names_tfidf)\n",
    "print('TF-IDF Matrix:\\n',x_tfidf.toarray())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec34e33",
   "metadata": {},
   "source": [
    "# One Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "869cebb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 1., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0., 0., 0., 0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 1., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 0.]), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 1.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "text=['govt is to blame for ethanols unpopularity oppgroup to meet in north west wa over rock ar','hacker gains access to eight million credit cards']\n",
    "\n",
    "tokens=[word for sent in text for word in sent.lower().split()]\n",
    "\n",
    "vocabulary=list(set(tokens))\n",
    "\n",
    "encoder=OneHotEncoder(categories=[vocabulary],sparse=False)\n",
    "\n",
    "one_hot_encoded=[]\n",
    "for sent in text:\n",
    "    sent_encoded=[]\n",
    "    for word in sent.lower().split():\n",
    "        word_index=vocabulary.index(word)\n",
    "        word_vector=np.zeros(len(vocabulary))\n",
    "        word_vector[word_index]=1\n",
    "        sent_encoded.append(word_vector)\n",
    "    one_hot_encoded.append(sent_encoded)\n",
    "    \n",
    "for sent in one_hot_encoded:\n",
    "    print(sent)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4dff2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b799c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
