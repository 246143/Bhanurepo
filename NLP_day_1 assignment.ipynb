{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b763c9",
   "metadata": {},
   "source": [
    "# 1. What is the purpose of text preprocessing in NLP, and why is it essential before analysis?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3bc0541e",
   "metadata": {},
   "source": [
    "Text preprocessing is a crucial step in Natural Language Processing (NLP) that involves cleaning and transforming raw text data into a format that is suitable for analysis. \n",
    "The purpose of text preprocessing is to enhance the quality of the data and make it more suitable for machine learning models.\n",
    "\n",
    "\n",
    "Some of the reasons are:\n",
    "    1)Noise Reduction\n",
    "    2)Normalization\n",
    "    3)Tokenization\n",
    "    4)Removing Stop words\n",
    "    5)Stemming and Lemmatization\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f81413c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\my\n",
      "[nltk_data]     pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\my\n",
      "[nltk_data]     pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\my\n",
      "[nltk_data]     pc\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text preprocess essenti step nlp analyz understand natur languag .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Join the words back into a sentence\n",
    "    preprocessed_text = ' '.join(words)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "# Example usage\n",
    "raw_text = \"Text preprocessing is an essential step in NLP for analyzing and understanding natural language.\"\n",
    "preprocessed_text = preprocess_text(raw_text)\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9216ec78",
   "metadata": {},
   "source": [
    "# 2. Describe tokenization in NLP and explain its significance in text processing.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe7b5193",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking down a text into individual units, called tokens. \n",
    "These tokens can be words, subwords, or even characters, depending on the granularity of the tokenization.\n",
    "Tokenization is a fundamental step in natural language processing (NLP) and text processing in general. \n",
    "It plays a crucial role in converting raw text into a format that is suitable for analysis, machine learning, and other NLP tasks.\n",
    "\n",
    "Significance in Text Processing:\n",
    "    1)Text Representation\n",
    "    2)Feature Extraction\n",
    "    3)Text Understanding\n",
    "    4)Vocabulary Creation\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efc82733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words: ['Tokenization', 'is', 'an', 'important', 'step', 'in', 'natural', 'language', 'processing', '.', 'It', 'breaks', 'down', 'text', 'into', 'individual', 'units', ',', 'such', 'as', 'words', 'or', 'sentences', '.']\n",
      "Tokenized Sentences: ['Tokenization is an important step in natural language processing.', 'It breaks down text into individual units, such as words or sentences.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\my\n",
      "[nltk_data]     pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Example text\n",
    "text = \"Tokenization is an important step in natural language processing. It breaks down text into individual units, such as words or sentences.\"\n",
    "\n",
    "# Tokenize into words\n",
    "words = word_tokenize(text)\n",
    "print(\"Tokenized Words:\", words)\n",
    "\n",
    "# Tokenize into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Tokenized Sentences:\", sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e868ec8",
   "metadata": {},
   "source": [
    "# 3. What are the differences between stemming and lemmatization in NLP? When would you choose one over the other ?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40b574c6",
   "metadata": {},
   "source": [
    "Stemming and lemmatization are both techniques used in natural language processing (NLP) to reduce words to their base or root form. However, they differ in their approaches and the results they produce.\n",
    "\n",
    "Stemming involves removing or cutting off the suffix from a word to obtain its root form. \n",
    "The resulting stems may not be actual words.\n",
    "\n",
    "Running -> Run\n",
    "Jumps -> Jump\n",
    "Swimming -> Swim\n",
    "\n",
    "\n",
    "Lemmatization involves reducing words to their base or root form, but the result is an actual word (lemma) with a valid meaning.\n",
    "\n",
    "Running -> Run\n",
    "Jumps -> Jump\n",
    "Swimming -> Swim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c27d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'jump', 'swim']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"jumps\", \"swimming\"]\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c2c107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'jump', 'swimming']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"jumps\", \"swimming\"]\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b522c61",
   "metadata": {},
   "source": [
    "# 4. Explain the concept of stop words and their role in text preprocessing. How do they impact NLP tasks ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdddb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stop words are commonly used words in a language that are often removed during the preprocessing of text data in natural language processing (NLP) tasks.\n",
    "These words are generally considered to be of little value for certain NLP tasks because they occur frequently in the language and do not contribute much to the overall meaning of a document.\n",
    "Examples of stop words in English include \"the,\" \"and,\" \"is,\" \"in,\" etc.\n",
    "\n",
    "\n",
    "Role of Stop Words in Text Preprocessing:\n",
    "\n",
    "1)Reducing Dimensionality:\n",
    "2)Improving efficiency\n",
    "3)Improving model performance\n",
    "\n",
    "Impact on NLP Tasks:\n",
    "\n",
    "1)Testing Classification\n",
    "2)information retrieval\n",
    "3)topic modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "742b4843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: This is an example sentence with some stop words.\n",
      "After Removing Stop Words: example sentence stop words .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\my\n",
      "[nltk_data]     pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\my\n",
      "[nltk_data]     pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"This is an example sentence with some stop words.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"After Removing Stop Words:\", ' '.join(filtered_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9074f7f",
   "metadata": {},
   "source": [
    "# 5. How does the process of removing punctuation contribute to text preprocessing in NLP? What are its benefits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Removing punctuation is an essential step in text preprocessing for natural language processing (NLP). \n",
    "Punctuation marks, such as commas, periods, exclamation points, and others, do not usually contribute to the semantic meaning of a text in many NLP tasks.\n",
    "\n",
    "Benefits of Removing Punctuation in Text Preprocessing:\n",
    "\n",
    "1)Reducing Noise\n",
    "2)Improving Tokenization\n",
    "3)Simplifying\n",
    "4)Uniformly in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ce4ac41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: This is an example sentence, showing the use of punctuation! It includes commas, periods, and exclamation marks.\n",
      "After Removing Punctuation: This is an example sentence showing the use of punctuation It includes commas periods and exclamation marks\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Example sentence with punctuation\n",
    "sentence = \"This is an example sentence, showing the use of punctuation! It includes commas, periods, and exclamation marks.\"\n",
    "\n",
    "# Remove punctuation\n",
    "cleaned_sentence = sentence.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"After Removing Punctuation:\", cleaned_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b320a4eb",
   "metadata": {},
   "source": [
    "# 6. Discuss the importance of lowercase conversion in text preprocessing. Why is it a common step in NLP tasks?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4441cf13",
   "metadata": {},
   "source": [
    "converting text to lowercase is a common and important step in text preprocessing for natural language processing (NLP) tasks.\n",
    "This process involves changing all the letters in the text to lowercase.\n",
    "\n",
    "\n",
    "\n",
    "Importance of Lowercase Conversion in Text Preprocessing:\n",
    "1)Consistency\n",
    "2)Normalization\n",
    "3)Reducing Vocabulary size\n",
    "4)Improved generalization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d2908be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: This is AJKDFJSDFKNSDND an Example  IFHODHOSsigso GSPOsentence with MiXeD cases.\n",
      "After Lowercasing: this is ajkdfjsdfknsdnd an example  ifhodhossigso gsposentence with mixed cases.\n"
     ]
    }
   ],
   "source": [
    "# Example sentence with mixed cases\n",
    "sentence = \"This is AJKDFJSDFKNSDND an Example  IFHODHOSsigso GSPOsentence with MiXeD cases.\"\n",
    "\n",
    "# Convert to lowercase\n",
    "lowercased_sentence = sentence.lower()\n",
    "\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"After Lowercasing:\", lowercased_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84934812",
   "metadata": {},
   "source": [
    "# 7. Explain the term \"vectorization\" concerning text data. How does techniques like CountVectorizer contribute to text preprocessing in NLP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea887cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorization in the context of text data refers to the process of converting a collection of text documents into numerical vectors.\n",
    "In NLP, this is a crucial step as machine learning models typically require numerical input. Vectorization allows us to represent text data in a format that can be used for various tasks such as classification, clustering, and regression.\n",
    "\n",
    "CountVectorizer is one of the techniques for vectorization in NLP. \n",
    "It converts a collection of text documents to a matrix of token counts. Each row of the matrix represents a document, and each column represents a unique word in the corpus.\n",
    "The entries of the matrix are the counts of the occurrences of words in the corresponding documents.\n",
    "\n",
    "\n",
    "How CountVectorizer Contributes to Text Preprocessing in NLP:\n",
    "1)word frequency representation\n",
    "2)sparse matrix\n",
    "3)bag of words model\n",
    "4)feature extraction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcf7a515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "CountVectorizer Output:\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\my pc\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example corpus\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "\n",
    "# Create the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the sparse matrix to a dense array for better visibility\n",
    "dense_array = X.toarray()\n",
    "\n",
    "# Display the feature names and the resulting matrix\n",
    "print(\"Feature Names:\", vectorizer.get_feature_names())\n",
    "print(\"CountVectorizer Output:\")\n",
    "print(dense_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e3b97",
   "metadata": {},
   "source": [
    "# 8. Describe the concept of normalization in NLP. Provide examples of normalization techniques used in text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7371bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Normalization in the context of natural language processing (NLP) refers to the process of standardizing and transforming text data to a common format, making it more consistent and easier to work with. \n",
    "Normalization helps in reducing noise, handling variations in text, and improving the performance of NLP mo\n",
    "\n",
    "Here are some common normalization techniques used in text preprocessing:\n",
    "\n",
    "1)Lower casing\n",
    "2)stemming\n",
    "3)lemmatizaton\n",
    "4)Removing accents\n",
    "5)Removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7412b4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: This is an Example with Mixed Cases.\n",
      "After Lowercasing: this is an example with mixed cases.\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an Example with Mixed Cases.\"\n",
    "normalized_text = text.lower()\n",
    "print(\"Original Text:\", text)\n",
    "print(\"After Lowercasing:\", normalized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "238175d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['running', 'jumps', 'swimming']\n",
      "After Stemming: ['run', 'jump', 'swim']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"jumps\", \"swimming\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(\"Original Words:\", words)\n",
    "print(\"After Stemming:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39dc4d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['running', 'jumps', 'swimming']\n",
      "After Lemmatization: ['running', 'jump', 'swimming']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"jumps\", \"swimming\"]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(\"Original Words:\", words)\n",
    "print(\"After Lemmatization:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7336678a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Café au Lait\n",
      "After Removing Accents: Cafe au Lait\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "text = \"Café au Lait\"\n",
    "normalized_text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "print(\"Original Text:\", text)\n",
    "print(\"After Removing Accents:\", normalized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cda49aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: This is an example sentence with @#$ special characters!\n",
      "After Removing Special Characters: This is an example sentence with  special characters\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"This is an example sentence with @#$ special characters!\"\n",
    "normalized_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "print(\"Original Text:\", text)\n",
    "print(\"After Removing Special Characters:\", normalized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a082c74b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
