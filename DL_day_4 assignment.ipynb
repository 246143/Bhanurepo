{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f9850ea",
   "metadata": {},
   "source": [
    "#  Build a Neural network with Hyper-parameter fine tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c3365a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from keras_tuner import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "555b9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trails=2 \n",
    "epochs=3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19bfe2f",
   "metadata": {},
   "source": [
    "# Get the fashion_MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c59ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "(xtrain,ytrain),(xval,yval)=fashion_mnist.load_data()\n",
    "xtrain=np.expand_dims(xtrain.astype('float32')/255.,-1)\n",
    "xval=np.expand_dims(xval.astype('float32')/255.,-1)\n",
    "ytrain=to_categorical(ytrain,10)\n",
    "yval=to_categorical(yval,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a91499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    \"\"\"\n",
    "    Function to build a TF model based on hyperparameter values\n",
    "    \n",
    "    Args:\n",
    "    hp (hyperparameter): values\n",
    "    \n",
    "    Returns:\n",
    "    Model: compiled model\n",
    "    \"\"\"\n",
    "    num_layers = hp.Int('num_layers', 2, 8, default=6)\n",
    "    lr = hp.Choice('learning_rate', [0.0001, 0.05])\n",
    "    inputs = layers.Input(shape=(28, 28, 1))\n",
    "    x = inputs\n",
    "    for idx in range(num_layers):\n",
    "        idx_str = str(idx)\n",
    "        filters = hp.Int('filters_' + idx_str, 32, 256, step=32, default=64)\n",
    "        x = layers.Conv2D(filters=filters, kernel_size=3, padding='same', activation='relu')(x)\n",
    "\n",
    "        if x.shape[1] >= 8:\n",
    "            pool_type = hp.Choice('pool_' + idx_str, values=['max', 'avg'])\n",
    "            if pool_type == 'max':\n",
    "                x = layers.MaxPooling2D(2)(x)\n",
    "            elif pool_type == 'avg':\n",
    "                x = layers.AveragePooling2D(2)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "    # Build model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c47e9e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from .\\FASHION_MNIST_results\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner=RandomSearch(build_model,objective='val_accuracy',max_trials=Trails,\n",
    "                  project_name='FASHION_MNIST_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c268e0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 10\n",
      "num_layers (Int)\n",
      "{'default': 6, 'conditions': [], 'min_value': 2, 'max_value': 8, 'step': 1, 'sampling': 'linear'}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.0001, 'conditions': [], 'values': [0.0001, 0.05], 'ordered': True}\n",
      "filters_0 (Int)\n",
      "{'default': 64, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
      "pool_0 (Choice)\n",
      "{'default': 'max', 'conditions': [], 'values': ['max', 'avg'], 'ordered': False}\n",
      "filters_1 (Int)\n",
      "{'default': 64, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
      "pool_1 (Choice)\n",
      "{'default': 'max', 'conditions': [], 'values': ['max', 'avg'], 'ordered': False}\n",
      "filters_2 (Int)\n",
      "{'default': 64, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
      "filters_3 (Int)\n",
      "{'default': 64, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
      "filters_4 (Int)\n",
      "{'default': 64, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
      "filters_5 (Int)\n",
      "{'default': 64, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f049471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 96)        960       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 96)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 192)       166080    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 7, 7, 192)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 224)         387296    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 10976)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                109770    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 664106 (2.53 MB)\n",
      "Trainable params: 664106 (2.53 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model=tuner.get_best_models(num_models=1)[0]\n",
    "best_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab2562c",
   "metadata": {},
   "source": [
    "# Build an image classifier model with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8891868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "486b94f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data\\train_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|██████████████████████████▌                                   | 78010277/182040794 [2:09:21<2:52:30, 10050.62it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File not found or corrupted.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      2\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m))\n\u001b[0;32m      4\u001b[0m ])\n\u001b[0;32m      6\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m----> 7\u001b[0m trainset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mSVHN(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m                                      download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m      9\u001b[0m trainloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(trainset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     10\u001b[0m                                           shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     12\u001b[0m testset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mSVHN(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m                                     download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\svhn.py:68\u001b[0m, in \u001b[0;36mSVHN.__init__\u001b[1;34m(self, root, split, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_md5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_list[split][\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\svhn.py:126\u001b[0m, in \u001b[0;36mSVHN.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     md5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_list[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit][\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m--> 126\u001b[0m     download_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename, md5)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\utils.py:155\u001b[0m, in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# check integrity of downloaded file\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_integrity(fpath, md5):\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found or corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: File not found or corrupted."
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "batch_size = 4\n",
    "trainset = torchvision.datasets.SVHN(root='./data', split='train',\n",
    "                                     download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.SVHN(root='./data', split='test',\n",
    "                                    download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=1)\n",
    "\n",
    "classes = tuple(str(i) for i in range(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c8a41d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Example random data\n",
    "train_data = torch.randn((100, 3, 32, 32))  # Assuming image data with shape (3, 32, 32)\n",
    "train_labels = torch.randint(0, 10, (100,))\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 10\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Rest of your code using trainloader\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "# ... rest of the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "333100b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a830930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba7d86a",
   "metadata": {},
   "source": [
    "# Train The network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f317d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab7199ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './svhn_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca05bba",
   "metadata": {},
   "source": [
    "# Test the network on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "433a06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Example random data\n",
    "train_data = torch.randn((100, 3, 32, 32))  # Assuming image data with shape (3, 32, 32)\n",
    "train_labels = torch.randint(0, 10, (100,))\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 10\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Rest of your code using trainloader\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24a0f8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b0cdcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4c0f0a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Print the predicted classes\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses[predicted[j]]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(predicted))))\n",
      "Cell \u001b[1;32mIn[30], line 8\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Print the predicted classes\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses[predicted[j]]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(predicted))))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ae0378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Example random data\n",
    "train_data = torch.randn((100, 3, 32, 32))  # Assuming image data with shape (3, 32, 32)\n",
    "train_labels = torch.randint(0, 10, (100,))\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 10\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Rest of your code using trainloader\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "015f9fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Example random data\n",
    "train_data = torch.randn((100, 3, 32, 32))  # Assuming image data with shape (3, 32, 32)\n",
    "train_labels = torch.randint(0, 10, (100,))\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 10\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Rest of your code using trainloader\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f66cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
